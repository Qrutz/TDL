PID,QuestionID,Subtheme,Codes,QuoteCandidate,Text
P01,Q9_followup,General transparency issues,feedback_issues,N,"It happens occasionally. Sometimes tasks are rejected without explanation or feedback. A few platforms offer an appeal process, but it's slow and rarely favors the worker. The lack of transparency makes it hard to improve or feel secure in the work."
P01,Q11_describe,Data usage transparency,"unclear_processes; feedback_issues; black_box_process; clear_processes",Y,"Most platforms provide minimal information about how tasks are reviewed or why certain submissions are rejected. Payment rules are vague, and sometimes task instructions are unclear or change without notice. There's little explanation about how our data is used or how quality is judged. The lack of clear feedback or appeal processes makes it feel like a black boxâ€”you're doing the work, but you don't know how it's being evaluated or where it ends up."
P02,Q11_describe,Positive transparency experience,clear_processes,N,"The system is very clear as it awaits review for ratings"
P03,Q11_describe,General transparency issues,,N,"some of the reviews take a very long time to process very short tasks"
P03,Q12_challenges,Data usage transparency,"unclear_processes; clear_processes",N,"it is sometimes unclear what the data is being used for"
P04,Q11_describe,Positive transparency experience,clear_processes,N,"the acceptance is fair and the rating system is good."
P05,Q11_describe,Positive transparency experience,clear_processes,N,"I do tasks willingly knowing exactly how much I would be paid"
P07,Q11_describe,Positive transparency experience,clear_processes,Y,"The platform is generally very good in explaining how tasks need to be completed in order to be accepted. And it's also generally good at explaining payment rules, though the pay rates are usually around minimum wage or less. The platform does pay almost immediately when a cash out is requested."
P08,Q11_describe,Data usage transparency,unclear_processes,Y,"There is little transparency as to how the data will be used and the nature of Prolific only tends to give an outcome for the individual task. It is possible you may be paid for a task but then removed from a white label group but the mechanics of this are not transparent at all - the work either appears on your dashboard or it does not!"
P09,Q11_describe,Positive transparency experience,clear_processes,Y,"Task acceptance was straightforward, each task type was explained clearly before I made my workload decisions, and the expectations were well-defined. The interface made it easy to understand how many tasks I could choose at each wage rate, which helped in making informed decisions."
P10,Q11_describe,Opaque rejection processes,"rejection_issues; rating_issues",N,"rating systems is not transparent, we get rejections that reseachers fail to justify"
P11,Q11_describe,Positive transparency experience,clear_processes,N,"They provide very detailed instructions and how you can contact researchers to get better feedback."
P15,Q11_describe,Positive transparency experience,clear_processes,N,"Prolific always paid well and on time"
P16,Q11_describe,Positive transparency experience,transparency_positive,Y,"Most of the platforms are very transparent as regarding the pay per hour. we just choose to work with that regardless"
P17,Q11_describe,Positive transparency experience,clear_processes,N,"The studies provide clear instructions on the participation requirements and creteria"
P18,Q11_describe,Positive transparency experience,clear_processes,N,"The study rules and method of compensation i quite clear just sometimes a study is underpaying"
P19,Q11_describe,Rating system issues,"rating_issues; clear_processes",Y,"The platform is clear about task acceptance and payment but the rating system and design choices could be explained better to avoid confusion and ensure fair evaluations"
P20,Q11_describe,Positive transparency experience,clear_processes,N,"Prolific shows the time the task will take and how much it is,"
P21,Q11_describe,Data usage transparency,,Y,"before starting the study everything is explained and how my data will be used and I always have an option to choose if I want my data to be used"
P22,Q11_describe,Positive transparency experience,clear_processes,Y,"I have not encountered any bad experience. some of the tasks are underpaying and most pay a fair amount for the work required."
P23,Q11_describe,Opaque rejection processes,rejection_issues,Y,"They are not clear on task acceptance because sometimes you get rejected but the reason is not clear."
P24,Q11_describe,Positive transparency experience,clear_processes,Y,"The platforms has always paid me and if the survey doesn't pay within 22 days, the platform pays automatically."
P25,Q11_describe,Positive transparency experience,clear_processes,N,"All tasks are cleared before performance, so that you have a chance to reject or accept the task."
P26,Q11_describe,Opaque rejection processes,rejection_issues,N,"One can get a rejection and never get to know why"
P27,Q11_describe,Positive transparency experience,transparency_positive,Y,"I WOULD MOSTLY FOCUS ON PROLIFIC, IT IS VERY TRANSPARENT WHEN IT COMES TO COMPENSATION, LIKE YOU TAKE ON A STUDY KNOWING VERY WELL HOW MUCH YOU'RE GOING TO GET PAID"
P28,Q9_followup,Opaque rejection processes,rejection_issues,N,"Only on a few occasions I have been rejected, but most of the time the reasons are vague, mainly the feedback can take like 2-3 weeks and I probably already forgotten what that task were. The feedback is quite appalling, sometimes the reasons don't even make sense. Like they approve 95% but the one task is not approved. I don't bother appealing, as I accept as part of the job. But if the platform rejects too many time, I will leave the platform and not do it again."
P28,Q11_describe,General transparency issues,feedback_issues,Y,"The work we do is quite transparent in terms of instructions and how we are paid. Usually per tasks or by the hour. However, when it comes to feedback most times it is very poor, bans, temporary bans, account is limited and some tasks rejected, you do not know why. And most people would simply go to forums like reddit to see if anyone was in the same situation. Unlike a real job, someone would advise you at least you did this wrong or you could do better, none here. Its like working in a silo, the moderators don't want to deal with you. Sending feedback or contacting them is useless, no one ever sends you anything. I simply take it that, you either understand the work or you don't. But you know it is what it is I guess."
P29,Q11_describe,Positive transparency experience,clear_processes,Y,"The platform clearly outlined task acceptance, with intuitive design choices, transparent rating systems, and straightforward payment rules, making the experience seamless"
P30,Q11_describe,Opaque rejection processes,rejection_issues,Y,"They are not clear on task acceptance. some researchers can just choose to reject the submission and them give you a lame reason like "" the response was automatically flagged"""
P31,Q11_describe,Positive transparency experience,transparency_positive,N,"The platform is transparent on how the researchers are paying us and the expected hourly rates"
P31,Q12_challenges,Opaque rejection processes,rejection_issues,Y,"Some researchers just reject submissions without giving a clear reason for it and they wont be available even when you try to reach them"
P32,Q12_challenges,Instruction clarity issues,unclear_processes,N,"Sometimes the instructions can be a little unclear"
P33,Q11_describe,Data usage transparency,unclear_processes,N,"Most hits aren't clear how they are reviewed or what it is used for."
P35,Q9_followup,Opaque rejection processes,"rejection_issues; rating_issues; feedback_issues; clear_processes",Y,"Ah, I see now! You're referring to a specific experience with a task rejection or going unpaid. If you've had this happen, it can definitely feel frustrating. Usually, when a task gets rejected or you're not paid for completed work, the platform should ideally give you a clear reason. Sometimes, it could be due to errors in the way the task was completed, like missing a specific instruction or misunderstanding the requirements. Other times, platforms might have vague rejection reasons that don't give much feedback. Did they provide any explanation for the rejection, or were you just left in the dark? Was there any opportunity to appeal or dispute the decision? It can be tough if you feel like there's no way to challenge or fix an issue that you may not have fully understood at the time."
P35,Q11_describe,General transparency issues,,N,"yes"
P36,Q11_describe,Rating system issues,"unclear_processes; unclear_criteria; rating_issues",Y,"The platform is often unclear about key aspects of the work. Task acceptance criteria aren't always explained, so it's hard to know why a task is rejected or accepted. The rating systems can feel inconsistent, and sometimes it's not clear how ratings are calculated or how they affect future opportunities."
P36,Q12_challenges,General transparency issues,unclear_processes,Y,"he biggest challenges are unclear task rules, unfair rejections, and lack of transparency. Disturbing content can cause emotional strain, and there's little support or way to dispute decisions. It often feels like the platform prioritizes speed over worker well-being."
P37,Q9_followup,Opaque rejection processes,"rejection_issues; rating_issues; feedback_issues; clear_processes",Y,"Ah, I see now! You're referring to a specific experience with a task rejection or going unpaid. If you've had this happen, it can definitely feel frustrating. Usually, when a task gets rejected or you're not paid for completed work, the platform should ideally give you a clear reason. Sometimes, it could be due to errors in the way the task was completed, like missing a specific instruction or misunderstanding the requirements. Other times, platforms might have vague rejection reasons that don't give much feedback. Did they provide any explanation for the rejection, or were you just left in the dark? Was there any opportunity to appeal or dispute the decision? It can be tough if you feel like there's no way to challenge or fix an issue that you may not have fully understood at the time."
P37,Q11_describe,General transparency issues,,N,"yes"
P37,Q12_challenges,Instruction clarity issues,"unclear_processes; black_box_process",Y,"Platform Usability and Confusing Rules The platforms that offer data annotation jobs often come with poorly designed user interfaces or confusing, inconsistent guidelines. If I were working on a platform like this, unclear instructions could lead to frustration or mistakes. The work might also feel like a ""black box,"" with little insight into how my tasks are being used or evaluated in the bigger picture."
P38,Q11_describe,General transparency issues,unclear_processes,Y,"Some platforms provide a general idea of how tasks are reviewed or paid. I usually understand the basic rules, but there are times when a task gets rejected or a score drops without a clear explanation. I think need more transparency. Because It would definitely help build trust"
P38,Q12_challenges,General transparency issues,feedback_issues,Y,"I think the biggest challenges is the lack of feedback. Because timely feedback when tasks are rejected would help us improve and feel valued. Dealing with disturbing content without warnings is emotionally hard."
P40,Q11_describe,Rating system issues,"unclear_criteria; rating_issues",Y,"the clarity of task descriptions and acceptance criteria, the transparency of the rating system and how it is used, the user-friendliness of the platform's design, and the explicitness of the payment structure"
P43,Q12_challenges,Instruction clarity issues,unclear_processes,Y,"One major challenge as a data annotator is dealing with vague or inconsistent guidelines, which can create confusion and lead to unfair evaluations or rejected work. Additionally, the emotional strain of labeling sensitive or disturbing content without proper support can take a serious toll over time."
P44,Q11_describe,Data usage transparency,unclear_processes,Y,"I think overall, many data labeling platforms exhibit low to moderate transparency regarding task review processes, compensation, and usage of workers' contributions. While some platforms are making efforts to improve, workers often face challenges related to unclear communication, inconsistent task availability, and delayed or withheld payments."
P45,Q11_describe,Positive transparency experience,clear_processes,Y,"What I appreciate most is when a task post includes a solid, checkable brief: clear deliverables, milestones, due dates, required skills, and any constraints (budget, format, language, etc.). When acceptance criteria are explicit, it's much easier to know when I've done enough to get paid. I like when there's a defined acceptance process. I'm less happy when posts are vague. If I have to guess what done looks like, I end up delivering something the client didn't want, or I delay submitting work while I chase clarifications. A good platform helps by prompting the client to fill in key fields or by offering a standard template for acceptance criteria."
P45,Q12_challenges,Instruction clarity issues,unclear_processes,Y,"Ambiguity in rules and task instructions Guidelines are often unclear or change without clear communication. I end up guessing, which can lead to inconsistent labels and frustration when QA piping in penalties."
P46,Q11_describe,Rating system issues,rating_issues,Y,"The platform provides some information about how tasks are reviewed and how payment works, but not always consistently. Sometimes it's clear why a task is accepted or rejected, and other times the reasons are vague. Rating systems and payment rules are partially explained, but more detailed guidance would help make the process feel fairer and more predictable."
P47,Q11_describe,Opaque rejection processes,"rejection_issues; rating_issues",Y,"Task instructions are usually clear, and the payment system is straightforward once work is approved. However, the rating and rejection process could be more transparent, with better options to dispute unfair decisions."
P48,Q11_describe,Opaque rejection processes,"rejection_issues; unclear_criteria; rating_issues",Y,"Amazon Mechanical Turk is generally clear about payment rules and deadlines, but the criteria for task acceptance and requester ratings can be vague. The platform's design is simple to navigate, yet it leaves workers guessing about how decisions on rejections or bonuses are made."
P50,Q11_describe,Opaque rejection processes,"unclear_processes; rejection_issues; unclear_criteria; rating_issues; feedback_issues",Y,"Here's a practical guide to assess how clear a platform is about task acceptance, rating systems, design choices, and payment rules, plus a reusable template you can apply. Task acceptance clarity What to look for Explicit acceptance criteria: what constitutes a valid task, required inputs, scope, and limits. Submission requirements: how to post, what fields are required, file types, word counts, templates, and examples. Review process: who reviews, expected timeframes, what happens after submission (approval, rejection, or requests for changes), and how feedback is delivered. Disallowed tasks: a clear, current list of prohibited content or activities. Re-submission rules: whether you can revise and re-submit, and under what conditions. Status updates: how task status is communicated (e.g., submitted, under review, accepted, rejected, needs revision) and how to track progress. Escalation and support: where to get help if a task is rejected or unclear. What good looks like"
